Found 3 GPU(s) available for training
  GPU 0: NVIDIA A100 80GB PCIe
  GPU 1: NVIDIA A100 80GB PCIe
  GPU 2: NVIDIA RTX A4000
Using device: cuda:0
Loading LIBERO dataset from /home/toramoto/toramoto/vla/LIBERO/libero/datasets/libero_spatial
Loaded 62250 samples from /home/toramoto/toramoto/vla/LIBERO/libero/datasets/libero_spatial
Train samples: 56025, Val samples: 6225
Loading VLA model from /home/toramoto/toramoto/miniVLA/vla_best_acc_0.8573.pt
Loading pretrained weights from /home/toramoto/toramoto/miniVLA/vla_best_acc_0.8573.pt
  ✓ Loaded: vision_encoder.mean
  ✓ Loaded: vision_encoder.std
  ✓ Loaded: vision_encoder.clip_vision.vision_model.embeddings.class_embedding
  ✓ Loaded: vision_encoder.clip_vision.vision_model.embeddings.patch_embedding.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.embeddings.position_embedding.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.pre_layrnorm.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.pre_layrnorm.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.0.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.1.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.2.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.3.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.4.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.5.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.6.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.7.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.8.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.9.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.10.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.self_attn.k_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.self_attn.k_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.self_attn.v_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.self_attn.v_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.self_attn.q_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.self_attn.q_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.self_attn.out_proj.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.self_attn.out_proj.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.layer_norm1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.layer_norm1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.mlp.fc1.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.mlp.fc1.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.mlp.fc2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.mlp.fc2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.layer_norm2.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.encoder.layers.11.layer_norm2.bias
  ✓ Loaded: vision_encoder.clip_vision.vision_model.post_layernorm.weight
  ✓ Loaded: vision_encoder.clip_vision.vision_model.post_layernorm.bias
  ✓ Loaded: llm.wte.weight
  ✓ Loaded: llm.wpe.weight
  ✓ Loaded: llm.h.0.ln_1.weight
  ✓ Loaded: llm.h.0.ln_1.bias
  ✓ Loaded: llm.h.0.attn.c_attn.weight
  ✓ Loaded: llm.h.0.attn.c_attn.bias
  ✓ Loaded: llm.h.0.attn.c_proj.weight
  ✓ Loaded: llm.h.0.attn.c_proj.bias
  ✓ Loaded: llm.h.0.ln_2.weight
  ✓ Loaded: llm.h.0.ln_2.bias
  ✓ Loaded: llm.h.0.mlp.c_fc.weight
  ✓ Loaded: llm.h.0.mlp.c_fc.bias
  ✓ Loaded: llm.h.0.mlp.c_proj.weight
  ✓ Loaded: llm.h.0.mlp.c_proj.bias
  ✓ Loaded: llm.h.1.ln_1.weight
  ✓ Loaded: llm.h.1.ln_1.bias
  ✓ Loaded: llm.h.1.attn.c_attn.weight
  ✓ Loaded: llm.h.1.attn.c_attn.bias
  ✓ Loaded: llm.h.1.attn.c_proj.weight
  ✓ Loaded: llm.h.1.attn.c_proj.bias
  ✓ Loaded: llm.h.1.ln_2.weight
  ✓ Loaded: llm.h.1.ln_2.bias
  ✓ Loaded: llm.h.1.mlp.c_fc.weight
  ✓ Loaded: llm.h.1.mlp.c_fc.bias
  ✓ Loaded: llm.h.1.mlp.c_proj.weight
  ✓ Loaded: llm.h.1.mlp.c_proj.bias
  ✓ Loaded: llm.h.2.ln_1.weight
  ✓ Loaded: llm.h.2.ln_1.bias
  ✓ Loaded: llm.h.2.attn.c_attn.weight
  ✓ Loaded: llm.h.2.attn.c_attn.bias
  ✓ Loaded: llm.h.2.attn.c_proj.weight
  ✓ Loaded: llm.h.2.attn.c_proj.bias
  ✓ Loaded: llm.h.2.ln_2.weight
  ✓ Loaded: llm.h.2.ln_2.bias
  ✓ Loaded: llm.h.2.mlp.c_fc.weight
  ✓ Loaded: llm.h.2.mlp.c_fc.bias
  ✓ Loaded: llm.h.2.mlp.c_proj.weight
  ✓ Loaded: llm.h.2.mlp.c_proj.bias
  ✓ Loaded: llm.h.3.ln_1.weight
  ✓ Loaded: llm.h.3.ln_1.bias
  ✓ Loaded: llm.h.3.attn.c_attn.weight
  ✓ Loaded: llm.h.3.attn.c_attn.bias
  ✓ Loaded: llm.h.3.attn.c_proj.weight
  ✓ Loaded: llm.h.3.attn.c_proj.bias
  ✓ Loaded: llm.h.3.ln_2.weight
  ✓ Loaded: llm.h.3.ln_2.bias
  ✓ Loaded: llm.h.3.mlp.c_fc.weight
  ✓ Loaded: llm.h.3.mlp.c_fc.bias
  ✓ Loaded: llm.h.3.mlp.c_proj.weight
  ✓ Loaded: llm.h.3.mlp.c_proj.bias
  ✓ Loaded: llm.h.4.ln_1.weight
  ✓ Loaded: llm.h.4.ln_1.bias
  ✓ Loaded: llm.h.4.attn.c_attn.weight
  ✓ Loaded: llm.h.4.attn.c_attn.bias
  ✓ Loaded: llm.h.4.attn.c_proj.weight
  ✓ Loaded: llm.h.4.attn.c_proj.bias
  ✓ Loaded: llm.h.4.ln_2.weight
  ✓ Loaded: llm.h.4.ln_2.bias
  ✓ Loaded: llm.h.4.mlp.c_fc.weight
  ✓ Loaded: llm.h.4.mlp.c_fc.bias
  ✓ Loaded: llm.h.4.mlp.c_proj.weight
  ✓ Loaded: llm.h.4.mlp.c_proj.bias
  ✓ Loaded: llm.h.5.ln_1.weight
  ✓ Loaded: llm.h.5.ln_1.bias
  ✓ Loaded: llm.h.5.attn.c_attn.weight
  ✓ Loaded: llm.h.5.attn.c_attn.bias
  ✓ Loaded: llm.h.5.attn.c_proj.weight
  ✓ Loaded: llm.h.5.attn.c_proj.bias
  ✓ Loaded: llm.h.5.ln_2.weight
  ✓ Loaded: llm.h.5.ln_2.bias
  ✓ Loaded: llm.h.5.mlp.c_fc.weight
  ✓ Loaded: llm.h.5.mlp.c_fc.bias
  ✓ Loaded: llm.h.5.mlp.c_proj.weight
  ✓ Loaded: llm.h.5.mlp.c_proj.bias
  ✓ Loaded: llm.h.6.ln_1.weight
  ✓ Loaded: llm.h.6.ln_1.bias
  ✓ Loaded: llm.h.6.attn.c_attn.weight
  ✓ Loaded: llm.h.6.attn.c_attn.bias
  ✓ Loaded: llm.h.6.attn.c_proj.weight
  ✓ Loaded: llm.h.6.attn.c_proj.bias
  ✓ Loaded: llm.h.6.ln_2.weight
  ✓ Loaded: llm.h.6.ln_2.bias
  ✓ Loaded: llm.h.6.mlp.c_fc.weight
  ✓ Loaded: llm.h.6.mlp.c_fc.bias
  ✓ Loaded: llm.h.6.mlp.c_proj.weight
  ✓ Loaded: llm.h.6.mlp.c_proj.bias
  ✓ Loaded: llm.h.7.ln_1.weight
  ✓ Loaded: llm.h.7.ln_1.bias
  ✓ Loaded: llm.h.7.attn.c_attn.weight
  ✓ Loaded: llm.h.7.attn.c_attn.bias
  ✓ Loaded: llm.h.7.attn.c_proj.weight
  ✓ Loaded: llm.h.7.attn.c_proj.bias
  ✓ Loaded: llm.h.7.ln_2.weight
  ✓ Loaded: llm.h.7.ln_2.bias
  ✓ Loaded: llm.h.7.mlp.c_fc.weight
  ✓ Loaded: llm.h.7.mlp.c_fc.bias
  ✓ Loaded: llm.h.7.mlp.c_proj.weight
  ✓ Loaded: llm.h.7.mlp.c_proj.bias
  ✓ Loaded: llm.h.8.ln_1.weight
  ✓ Loaded: llm.h.8.ln_1.bias
  ✓ Loaded: llm.h.8.attn.c_attn.weight
  ✓ Loaded: llm.h.8.attn.c_attn.bias
  ✓ Loaded: llm.h.8.attn.c_proj.weight
  ✓ Loaded: llm.h.8.attn.c_proj.bias
  ✓ Loaded: llm.h.8.ln_2.weight
  ✓ Loaded: llm.h.8.ln_2.bias
  ✓ Loaded: llm.h.8.mlp.c_fc.weight
  ✓ Loaded: llm.h.8.mlp.c_fc.bias
  ✓ Loaded: llm.h.8.mlp.c_proj.weight
  ✓ Loaded: llm.h.8.mlp.c_proj.bias
  ✓ Loaded: llm.h.9.ln_1.weight
  ✓ Loaded: llm.h.9.ln_1.bias
  ✓ Loaded: llm.h.9.attn.c_attn.weight
  ✓ Loaded: llm.h.9.attn.c_attn.bias
  ✓ Loaded: llm.h.9.attn.c_proj.weight
  ✓ Loaded: llm.h.9.attn.c_proj.bias
  ✓ Loaded: llm.h.9.ln_2.weight
  ✓ Loaded: llm.h.9.ln_2.bias
  ✓ Loaded: llm.h.9.mlp.c_fc.weight
  ✓ Loaded: llm.h.9.mlp.c_fc.bias
  ✓ Loaded: llm.h.9.mlp.c_proj.weight
  ✓ Loaded: llm.h.9.mlp.c_proj.bias
  ✓ Loaded: llm.h.10.ln_1.weight
  ✓ Loaded: llm.h.10.ln_1.bias
  ✓ Loaded: llm.h.10.attn.c_attn.weight
  ✓ Loaded: llm.h.10.attn.c_attn.bias
  ✓ Loaded: llm.h.10.attn.c_proj.weight
  ✓ Loaded: llm.h.10.attn.c_proj.bias
  ✓ Loaded: llm.h.10.ln_2.weight
  ✓ Loaded: llm.h.10.ln_2.bias
  ✓ Loaded: llm.h.10.mlp.c_fc.weight
  ✓ Loaded: llm.h.10.mlp.c_fc.bias
  ✓ Loaded: llm.h.10.mlp.c_proj.weight
  ✓ Loaded: llm.h.10.mlp.c_proj.bias
  ✓ Loaded: llm.h.11.ln_1.weight
  ✓ Loaded: llm.h.11.ln_1.bias
  ✓ Loaded: llm.h.11.attn.c_attn.weight
  ✓ Loaded: llm.h.11.attn.c_attn.bias
  ✓ Loaded: llm.h.11.attn.c_proj.weight
  ✓ Loaded: llm.h.11.attn.c_proj.bias
  ✓ Loaded: llm.h.11.ln_2.weight
  ✓ Loaded: llm.h.11.ln_2.bias
  ✓ Loaded: llm.h.11.mlp.c_fc.weight
  ✓ Loaded: llm.h.11.mlp.c_fc.bias
  ✓ Loaded: llm.h.11.mlp.c_proj.weight
  ✓ Loaded: llm.h.11.mlp.c_proj.bias
  ✓ Loaded: llm.ln_f.weight
  ✓ Loaded: llm.ln_f.bias
  ✓ Loaded: qformer.query_tokens
  ✓ Loaded: qformer.crossattention.in_proj_weight
  ✓ Loaded: qformer.crossattention.in_proj_bias
  ✓ Loaded: qformer.crossattention.out_proj.weight
  ✓ Loaded: qformer.crossattention.out_proj.bias
  ✓ Loaded: qformer.cross_ln.weight
  ✓ Loaded: qformer.cross_ln.bias
  ✓ Loaded: qformer.encoder.layers.0.self_attn.in_proj_weight
  ✓ Loaded: qformer.encoder.layers.0.self_attn.in_proj_bias
  ✓ Loaded: qformer.encoder.layers.0.self_attn.out_proj.weight
  ✓ Loaded: qformer.encoder.layers.0.self_attn.out_proj.bias
  ✓ Loaded: qformer.encoder.layers.0.linear1.weight
  ✓ Loaded: qformer.encoder.layers.0.linear1.bias
  ✓ Loaded: qformer.encoder.layers.0.linear2.weight
  ✓ Loaded: qformer.encoder.layers.0.linear2.bias
  ✓ Loaded: qformer.encoder.layers.0.norm1.weight
  ✓ Loaded: qformer.encoder.layers.0.norm1.bias
  ✓ Loaded: qformer.encoder.layers.0.norm2.weight
  ✓ Loaded: qformer.encoder.layers.0.norm2.bias
  ✓ Loaded: qformer.encoder.layers.1.self_attn.in_proj_weight
  ✓ Loaded: qformer.encoder.layers.1.self_attn.in_proj_bias
  ✓ Loaded: qformer.encoder.layers.1.self_attn.out_proj.weight
  ✓ Loaded: qformer.encoder.layers.1.self_attn.out_proj.bias
  ✓ Loaded: qformer.encoder.layers.1.linear1.weight
  ✓ Loaded: qformer.encoder.layers.1.linear1.bias
  ✓ Loaded: qformer.encoder.layers.1.linear2.weight
  ✓ Loaded: qformer.encoder.layers.1.linear2.bias
  ✓ Loaded: qformer.encoder.layers.1.norm1.weight
  ✓ Loaded: qformer.encoder.layers.1.norm1.bias
  ✓ Loaded: qformer.encoder.layers.1.norm2.weight
  ✓ Loaded: qformer.encoder.layers.1.norm2.bias
  ✓ Loaded: qformer.encoder.layers.2.self_attn.in_proj_weight
  ✓ Loaded: qformer.encoder.layers.2.self_attn.in_proj_bias
  ✓ Loaded: qformer.encoder.layers.2.self_attn.out_proj.weight
  ✓ Loaded: qformer.encoder.layers.2.self_attn.out_proj.bias
  ✓ Loaded: qformer.encoder.layers.2.linear1.weight
  ✓ Loaded: qformer.encoder.layers.2.linear1.bias
  ✓ Loaded: qformer.encoder.layers.2.linear2.weight
  ✓ Loaded: qformer.encoder.layers.2.linear2.bias
  ✓ Loaded: qformer.encoder.layers.2.norm1.weight
  ✓ Loaded: qformer.encoder.layers.2.norm1.bias
  ✓ Loaded: qformer.encoder.layers.2.norm2.weight
  ✓ Loaded: qformer.encoder.layers.2.norm2.bias
  ✓ Loaded: qformer.encoder.layers.3.self_attn.in_proj_weight
  ✓ Loaded: qformer.encoder.layers.3.self_attn.in_proj_bias
  ✓ Loaded: qformer.encoder.layers.3.self_attn.out_proj.weight
  ✓ Loaded: qformer.encoder.layers.3.self_attn.out_proj.bias
  ✓ Loaded: qformer.encoder.layers.3.linear1.weight
  ✓ Loaded: qformer.encoder.layers.3.linear1.bias
  ✓ Loaded: qformer.encoder.layers.3.linear2.weight
  ✓ Loaded: qformer.encoder.layers.3.linear2.bias
  ✓ Loaded: qformer.encoder.layers.3.norm1.weight
  ✓ Loaded: qformer.encoder.layers.3.norm1.bias
  ✓ Loaded: qformer.encoder.layers.3.norm2.weight
  ✓ Loaded: qformer.encoder.layers.3.norm2.bias
Loaded 404/410 layers from pretrained model
学習可能パラメータ: 117,055,495 / 241,496,071 (48.5%)
  Vision Encoder: 85,799,424
  Q-Former: 30,727,680
  LLM: 0
  Action Head: 528,391

============================================================
Epoch 1/20
============================================================
