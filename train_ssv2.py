#train_ssv2.py
import torch
from torch.utils.data import DataLoader, Subset
from datasets import load_dataset
import numpy as np
from collections import Counter, defaultdict
import pickle
import os
from pathlib import Path

from sample_vla import MiniVLA
from ssv2_dataset import UCF101MiniVLADataset
from collate_ucf101 import collate_ucf101
from Trainer import VLA_Trainer

def fast_stratified_sampling(dataset, target_size, seed=42, cache_dir="./cache"):
    """é«˜é€Ÿå±¤åˆ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨NumPyé…åˆ—ã§æœ€é©åŒ–"""
    np.random.seed(seed)
    Path(cache_dir).mkdir(exist_ok=True)
    cache_file = f"{cache_dir}/ucf101_labels_{len(dataset)}_{seed}.pkl"
    
    if os.path.exists(cache_file):
        print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­: {cache_file}")
        with open(cache_file, 'rb') as f:
            class_indices = pickle.load(f)
        print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿å®Œäº†")
    else:
        print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ©ãƒ™ãƒ«æƒ…å ±ã‚’æ§‹ç¯‰ä¸­...")
        all_labels = np.array(dataset['label'])
        print(f"  ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(all_labels)}")
        
        class_indices = defaultdict(list)
        for idx, label in enumerate(all_labels):
            class_indices[label].append(idx)
        
        class_indices = {k: np.array(v) for k, v in class_indices.items()}
        
        print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ä¸­: {cache_file}")
        with open(cache_file, 'wb') as f:
            pickle.dump(class_indices, f)
        print(f"âœ“ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†")
    
    num_classes = len(class_indices)
    samples_per_class = target_size // num_classes
    print(f"å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰{samples_per_class}ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾— (ç·è¨ˆ: {samples_per_class * num_classes})")
    
    selected_indices = []
    for class_id, indices in class_indices.items():
        available_samples = len(indices)
        take_samples = min(samples_per_class, available_samples)
        selected = np.random.choice(indices, size=take_samples, replace=False)
        selected_indices.extend(selected.tolist())
    
    print(f"âœ“ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº†: {len(selected_indices)}ã‚µãƒ³ãƒ—ãƒ«é¸æŠ")
    return selected_indices

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # A100 GPUç”¨ã®æœ€é©åŒ–è¨­å®š
    if device == "cuda":
        print("=== A100 GPUæœ€é©åŒ–è¨­å®š ===")
        torch.backends.cudnn.benchmark = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        print("âœ“ CuDNN benchmark enabled")
        print("âœ“ TF32 enabled for Tensor Cores")

    print("\n=== Stage 3: Vision + Q-Former + LLM (ç°¡æ˜“çµ±åˆ) ===")
    print("ç›®çš„: LLMçµ±åˆå¾Œã‚‚å­¦ç¿’ã§ãã‚‹ã‹æ¤œè¨¼")
    print("æˆ¦ç•¥: Visionç‰¹å¾´ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ã€ãƒ†ã‚­ã‚¹ãƒˆå½±éŸ¿ã‚’æœ€å°åŒ–\n")

    print("Loading UCF101...")
    ds_full_train = load_dataset("flwrlabs/ucf101", split="train")
    ds_full_test = load_dataset("flwrlabs/ucf101", split="test")
    
    print("\n=== é«˜é€Ÿå±¤åˆ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­ ===")
    train_indices = fast_stratified_sampling(ds_full_train, target_size=20000, seed=42)
    test_indices = fast_stratified_sampling(ds_full_test, target_size=5000, seed=42)
    
    ds_train = ds_full_train.select(train_indices)
    ds_val = ds_full_test.select(test_indices)
    
    print("\n=== ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°çµæœ ===")
    train_labels = np.array(ds_train['label'])
    val_labels = np.array(ds_val['label'])
    
    train_counter = Counter(train_labels)
    val_counter = Counter(val_labels)
    
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«æ•°: {len(set(train_labels))}")
    print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ©ãƒ™ãƒ«æ•°: {len(set(val_labels))}")

    print("\nLoading VLA Model with LLM...")
    # LLMã‚’æœ‰åŠ¹åŒ–ã—ã¦VLAãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
    model = MiniVLA(
        num_actions=101,
        use_llm=True,  # LLMã‚’ä½¿ç”¨
        vision_trainable=True,
        llm_trainable=True
    ).to(device)
    
    model.get_trainable_params()

    train_set = UCF101MiniVLADataset(ds_train, model.tokenizer)
    val_set = UCF101MiniVLADataset(ds_val, model.tokenizer)

    batch_size = 64
    print(f"\n=== Trainingè¨­å®š ===")
    print(f"Batch size: {batch_size}")
    
    # å±¤ã”ã¨ã®å­¦ç¿’ç‡ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®æˆåŠŸã‚’å‚è€ƒã«èª¿æ•´ï¼‰
    vision_params = list(model.vision_encoder.parameters())
    qformer_params = list(model.qformer.parameters())
    llm_params = [p for p in model.llm.parameters() if p.requires_grad]
    action_params = list(model.action_head.parameters())
    
    optimizer = torch.optim.AdamW([
        {'params': vision_params, 'lr': 1e-5, 'weight_decay': 0.01},  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒã˜
        {'params': qformer_params, 'lr': 5e-5, 'weight_decay': 0.01},  # æ…é‡ã«
        {'params': llm_params, 'lr': 1e-5, 'weight_decay': 0.01},  # æ…é‡ã«
        {'params': action_params, 'lr': 1e-3, 'weight_decay': 0.01}  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒã˜
    ])
    
    print(f"Optimizer: AdamW with layer-wise learning rates")
    print(f"  Vision Encoder: lr=1e-5")
    print(f"  Q-Former: lr=5e-5")
    print(f"  LLM: lr=1e-5")
    print(f"  Action Head: lr=1e-3")
    
    from torch.optim.lr_scheduler import CosineAnnealingLR
    num_epochs = 15  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šé•·ã‚ã«
    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-7)
    print(f"Scheduler: CosineAnnealingLR ({num_epochs} epochs)")

    train_loader = DataLoader(
        train_set, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=lambda batch: collate_ucf101(batch, model.tokenizer),
        num_workers=8,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True
    )
    val_loader = DataLoader(
        val_set, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=lambda batch: collate_ucf101(batch, model.tokenizer),
        num_workers=8,
        pin_memory=True,
        prefetch_factor=2,
        persistent_workers=True
    )

    trainer = VLA_Trainer(model, optimizer, device, use_amp=True)

    print(f"\n=== Trainingé–‹å§‹ ===")
    print("ç›®æ¨™: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³88.46%ã«è¿‘ã„æ€§èƒ½ã‚’é”æˆ")
    print("æœŸå¾…: åˆæœŸã‚¨ãƒãƒƒã‚¯ã§20%ä»¥ä¸Šã€æœ€çµ‚çš„ã«60%ä»¥ä¸Š\n")
    
    best_val_acc = 0.0
    
    for epoch in range(num_epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"{'='*60}")
        
        train_loss = trainer.train_epoch(train_loader)
        val_loss, val_acc = trainer.eval(val_loader)

        print(f"\nEpoch {epoch+1} Results:")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Val Loss:   {val_loss:.4f}")
        print(f"  Val Acc:    {val_acc:.4f} (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: 0.8846)")
        print(f"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # é€²æ—è©•ä¾¡
        if epoch == 0 and val_acc < 0.05:
            print("  âš ï¸  è­¦å‘Š: åˆæœŸã‚¨ãƒãƒƒã‚¯ã§ç²¾åº¦ãŒ5%æœªæº€ã§ã™")
        elif val_acc > 0.50:
            print(f"  âœ… è‰¯å¥½: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®{val_acc/0.8846*100:.1f}%ã«åˆ°é”")
        
        scheduler.step()
        
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            checkpoint_path = f'vla_best_acc_{val_acc:.4f}.pt'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss,
            }, checkpoint_path)
            print(f"  âœ“ Best model saved: {checkpoint_path}")
        
        if (epoch + 1) % 5 == 0:
            checkpoint_path = f'vla_checkpoint_epoch_{epoch+1}.pt'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss,
            }, checkpoint_path)
            print(f"  âœ“ Checkpoint saved: {checkpoint_path}")
    
    print(f"\n{'='*60}")
    print(f"VLA Training Complete!")
    print(f"Best Validation Accuracy: {best_val_acc:.4f}")
    print(f"Baseline Accuracy: 0.8846")
    print(f"Performance Ratio: {best_val_acc/0.8846*100:.1f}%")
    print(f"{'='*60}")
    
    if best_val_acc > 0.70:
        print("\nğŸ‰ æˆåŠŸï¼VLAãƒ¢ãƒ‡ãƒ«ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«è¿‘ã„æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸ")
        print("   æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: ã‚ˆã‚Šè¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚„ãƒ­ãƒœãƒƒãƒˆåˆ¶å¾¡ã¸ã®å¿œç”¨")
    elif best_val_acc > 0.40:
        print("\nâœ… æ”¹å–„ï¼LLMçµ±åˆå¾Œã‚‚å­¦ç¿’ã§ãã¦ã„ã¾ã™")
        print("   ã•ã‚‰ãªã‚‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ï¼š")
        print("   - ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™")
        print("   - Q-Formerã®æ§‹é€ ã‚’æœ€é©åŒ–")
        print("   - ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¿½åŠ ")
    else:
        print("\nâš ï¸  LLMçµ±åˆã«ã¾ã å•é¡ŒãŒã‚ã‚Šã¾ã™")
        print("   è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ï¼š")
        print("   - LLMãŒVisionæƒ…å ±ã‚’ä¸Šæ›¸ãã—ã¦ã„ã‚‹")
        print("   - Q-Formerã®åœ§ç¸®ãŒéåº¦")
        print("   - å­¦ç¿’ç‡ãŒä¸é©åˆ‡")

if __name__ == "__main__":
    main()
